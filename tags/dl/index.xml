<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>dl on Àlog</title>
    <link>https://socalesc.github.io/tags/dl/</link>
    <description>Recent content in dl on Àlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 17 Dec 2022 17:51:02 +0100</lastBuildDate><atom:link href="https://socalesc.github.io/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention and Context based Embeddings</title>
      <link>https://socalesc.github.io/posts/2022-12-17-attention/attention/</link>
      <pubDate>Sat, 17 Dec 2022 17:51:02 +0100</pubDate>
      
      <guid>https://socalesc.github.io/posts/2022-12-17-attention/attention/</guid>
      <description>Attention mechanisms are a type of techniques used in natural language processing (NLP) tasks that allow a model to focus on specific parts of the input when processing a sequence, rather than considering the entire sequence at once. These methods can improve the performance of the model by allowing it to efficiently process long sequences of text and make more accurate predictions.
To some extend, attention mechanisms are motivated by how human visual attention focuses on different regions of an image or how correlates words in a sentence.</description>
    </item>
    
  </channel>
</rss>
